"""
å®éªŒ3 v3ï¼šé•¿æ—¶é—´è¯­éŸ³è¾“å…¥çš„æ¸è¿›å¼æ€»ç»“æµ‹è¯•ï¼ˆä¼˜åŒ–ç‰ˆ - å¹¶è¡Œå¤„ç†ï¼‰

æ”¹è¿›ç‚¹ï¼š
1. ç®€åŒ–æ€»ç»“æ•°æ®ç»“æ„ï¼Œåªä¿ç•™summary
2. æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡éƒ½äº¤ç»™LLMè¯„åˆ†
3. æ¨¡æ‹ŸçœŸå®å»¶è¿Ÿï¼ˆè¾¹è¯´è¾¹æ€»ç»“ï¼‰
4. ä½¿ç”¨å¿«é€Ÿæ¨¡å‹åšæ€»ç»“ï¼ˆqwen3-8bï¼‰
5. ä½¿ç”¨14Bæ¨¡å‹ç”Ÿæˆå›å¤ï¼ˆqwen3-14bï¼‰
6. æµå¼è¾“å‡ºæœ€ç»ˆå›å¤ï¼Œè®°å½•é¦–tokenå»¶è¿Ÿ
7. è¯„ä¼°æ—¶é—´ä»ç”¨æˆ·è¾“å…¥å®Œæˆå¼€å§‹è®¡ç®—
8. å¹¶è¡Œè¿è¡Œä¸‰ä¸ªæ–¹æ³•ï¼ˆç”¨æˆ·åªè¯´ä¸€æ¬¡ï¼Œä¸‰ä¸ªagentåŒæ—¶å¤„ç†ï¼‰
"""

import os
import sys
import json
import time
import threading
from datetime import datetime
from typing import Dict, List, Tuple
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

sys.path.insert(0, str(Path(__file__).parent.parent))

from openai import OpenAI
from dotenv import load_dotenv
from rag_utils import EmbeddingService, VectorIndex
from data.fictional_knowledge_base import FICTIONAL_DOCUMENTS
from data.company_graph import convert_all_companies_to_documents
from experiments.incremental_summarizer_v2 import SimpleSummarizer
from experiments.incremental_summarizer_v3 import IncrementalRAGSummarizer

load_dotenv()

# æ¨¡å‹é…ç½®
API_KEY = os.getenv("QWEN_TOKEN")
BASE_URL = os.getenv("QWEN_API_BASE", "https://dashscope.aliyuncs.com/compatible-mode/v1")
SUMMARY_MODEL = "qwen3-8b"  # 8Bæ¨¡å‹ç”¨äºæ€»ç»“ï¼ˆå¿«é€Ÿï¼‰
RESPONSE_MODEL = "qwen3-14b"  # 14Bæ¨¡å‹ç”¨äºç”Ÿæˆå›å¤ï¼ˆé«˜è´¨é‡ï¼‰
EVAL_MODEL = "qwen3-14b"  # 14Bæ¨¡å‹ç”¨äºè¯„ä¼°


# ========== åŠ è½½æµ‹è¯•ç”¨ä¾‹ ==========

def load_test_cases() -> List[Dict]:
    """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
    test_file = Path(__file__).parent / "long_audio_test_cases_v2.json"
    with open(test_file, 'r', encoding='utf-8') as f:
        data = json.loads(f.read())
    return data["test_cases"]


# ========== LLMè¯„ä¼°å‡½æ•° ==========

def llm_evaluate_all(
    method_name: str,
    original_input: str,
    summary: str,
    rag_results: List[Dict],
    final_response: str,
    ground_truth: Dict,
    llm_client: OpenAI
) -> Dict:
    """
    ä½¿ç”¨LLMä¸€æ¬¡æ€§è¯„ä¼°æ‰€æœ‰æŒ‡æ ‡
    """
    eval_prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹è¯­éŸ³åŠ©æ‰‹çš„å¤„ç†è´¨é‡ã€‚

ã€åŸå§‹ç”¨æˆ·è¾“å…¥ã€‘ï¼ˆ{len(original_input)}å­—ï¼‰ï¼š
{original_input[:200]}...

ã€æ€»ç»“ç»“æœã€‘ï¼ˆ{len(summary)}å­—ï¼‰ï¼š
{summary}

ã€æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‘ï¼š
{chr(10).join([f"{i+1}. {doc.get('title', 'æ— æ ‡é¢˜')}" for i, doc in enumerate(rag_results[:3])])}

ã€æœ€ç»ˆå›å¤ã€‘ï¼š
{final_response[:300]}...

ã€æ ‡å‡†ç­”æ¡ˆå‚è€ƒã€‘ï¼š
- å…³é”®ä¿¡æ¯ç‚¹ï¼š{', '.join(ground_truth.get('key_points', [])[:5])}
- å…³é”®å®ä½“ï¼š{', '.join(ground_truth.get('entities', []))}
- åŸæ–‡å™ªéŸ³é¡¹æ•°é‡ï¼š{ground_truth.get('total_noise_count', 0)}

è¯·æŒ‰ä»¥ä¸‹æ ‡å‡†è¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰ï¼š

1. ä¿¡æ¯ä¿ç•™ç‡ï¼ˆ0-100åˆ†ï¼‰ï¼šæ€»ç»“æ˜¯å¦ä¿ç•™äº†æ‰€æœ‰å…³é”®ä¿¡æ¯
2. å™ªéŸ³è¿‡æ»¤ç‡ï¼ˆ0-100åˆ†ï¼‰ï¼šæ˜¯å¦æœ‰æ•ˆè¿‡æ»¤äº†å£è¯­è¯ã€å¯’æš„ç­‰æ— ç”¨ä¿¡æ¯
3. RAGç›¸å…³æ€§ï¼ˆ0-100åˆ†ï¼‰ï¼šæ£€ç´¢çš„æ–‡æ¡£æ˜¯å¦ä¸ç”¨æˆ·éœ€æ±‚ç›¸å…³
4. å›å¤è´¨é‡ï¼ˆ0-100åˆ†ï¼‰ï¼šå›å¤æ˜¯å¦å‡†ç¡®ã€å…¨é¢ã€ä¸“ä¸š
5. ç®€æ´åº¦ï¼ˆ0-100åˆ†ï¼‰ï¼šæ€»ç»“æ˜¯å¦ç®€æ´ï¼Œæ— å†—ä½™

è¯·æ ¹æ®å®é™…æƒ…å†µç»™å‡ºçœŸå®çš„åˆ†æ•°ï¼Œç„¶åè®¡ç®—æ€»åˆ†ï¼ˆ5é¡¹å¹³å‡å€¼ï¼‰ã€‚

è¿”å›JSONæ ¼å¼ï¼ˆä¸è¦markdownæ ‡è®°ï¼Œä¸è¦ç¤ºä¾‹æ•°å­—ï¼Œç»™å‡ºä½ çš„çœŸå®è¯„åˆ†ï¼‰ï¼š
{{
  "info_retention_score": <ä½ çš„è¯„åˆ†>,
  "noise_filtering_score": <ä½ çš„è¯„åˆ†>,
  "rag_relevance_score": <ä½ çš„è¯„åˆ†>,
  "response_quality_score": <ä½ çš„è¯„åˆ†>,
  "conciseness_score": <ä½ çš„è¯„åˆ†>,
  "total_score": <5é¡¹å¹³å‡å€¼>,
  "reasoning": "<è¯¦ç»†çš„è¯„åˆ†ç†ç”±>"
}}
"""

    try:
        response_obj = llm_client.chat.completions.create(
            model=EVAL_MODEL,
            messages=[{"role": "user", "content": eval_prompt}],
            temperature=0.1,
            stream=False,
            extra_body={"enable_thinking": False}
        )

        content = response_obj.choices[0].message.content.strip()

        # æ¸…ç†markdownæ ‡è®°
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]

        result = json.loads(content.strip())
        return result

    except Exception as e:
        print(f"LLMè¯„ä¼°å¤±è´¥: {e}")
        return {
            "info_retention_score": 50,
            "noise_filtering_score": 50,
            "rag_relevance_score": 50,
            "response_quality_score": 50,
            "conciseness_score": 50,
            "total_score": 50,
            "reasoning": f"è¯„ä¼°å¤±è´¥: {str(e)}"
        }


# ========== è¾…åŠ©å‡½æ•° ==========

def search_with_query_text(
    query_text: str,
    vector_index: VectorIndex,
    embedding_service: EmbeddingService,
    top_k: int = 5
) -> List[Dict]:
    """ä½¿ç”¨æ–‡æœ¬queryè¿›è¡Œå‘é‡æ£€ç´¢"""
    query_vector = embedding_service.embed_single(query_text)
    results = vector_index.search(query_vector, top_k=top_k)
    return [{"id": r["doc_id"], "title": r["title"], "content": r["content"]} for r in results]


def generate_response_streaming(
    prompt: str,
    llm_client: OpenAI
) -> Tuple[str, float, float, int]:
    """
    æµå¼ç”Ÿæˆå›å¤

    Returns:
        (å®Œæ•´å›å¤, é¦–tokenå»¶è¿Ÿ, æ€»ç”Ÿæˆæ—¶é—´, tokenæ•°)
    """
    start_time = time.time()
    first_token_time = None
    full_response = ""
    token_count = 0

    try:
        stream = llm_client.chat.completions.create(
            model=RESPONSE_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            stream=True,
            extra_body={"enable_thinking": False}
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                if first_token_time is None:
                    first_token_time = time.time() - start_time

                content = chunk.choices[0].delta.content
                full_response += content
                token_count += 1

        total_time = time.time() - start_time

        return full_response, first_token_time or 0, total_time, token_count

    except Exception as e:
        print(f"æµå¼ç”Ÿæˆå¤±è´¥: {e}")
        return f"ç”Ÿæˆå¤±è´¥: {str(e)}", 0, 0, 0


# ========== æ–¹æ³•1ï¼šBaselineï¼ˆç›´æ¥RAGï¼‰ ==========

def method1_baseline(
    full_text: str,
    vector_index: VectorIndex,
    embedding_service: EmbeddingService,
    llm_client: OpenAI,
    ground_truth: Dict,
    top_k: int = 5
) -> Dict:
    """æ–¹æ³•1ï¼šç›´æ¥ä½¿ç”¨å®Œæ•´çš„800å­—æ–‡æœ¬è¿›è¡ŒRAG"""
    # ç”¨æˆ·è¾“å…¥å®Œæˆæ—¶åˆ»
    input_complete_time = time.time()

    # 1. å‘é‡æ£€ç´¢
    rag_start = time.time()
    results = search_with_query_text(full_text, vector_index, embedding_service, top_k)
    rag_time = time.time() - rag_start

    # 2. æ„å»ºprompt
    rag_context = ""
    for i, doc in enumerate(results, 1):
        rag_context += f"\n[æ–‡æ¡£{i}] {doc.get('title', 'æ— æ ‡é¢˜')}\n"
        rag_context += f"{doc.get('content', 'æ— å†…å®¹')[:500]}\n"

    prompt = f"""ç”¨æˆ·æŸ¥è¯¢ï¼š
{full_text}

ç›¸å…³ä¿¡æ¯ï¼š
{rag_context}

è¯·ç»™å‡ºä¸“ä¸šã€å‡†ç¡®çš„å›å¤ï¼š
"""

    # 3. æµå¼ç”Ÿæˆå›å¤
    gen_start = time.time()
    final_response, ttft, gen_time, token_count = generate_response_streaming(prompt, llm_client)

    # 4. LLMè¯„ä¼°
    eval_result = llm_evaluate_all(
        "baseline",
        full_text,
        full_text,  # baselineæ²¡æœ‰summaryï¼Œç”¨åŸæ–‡
        results,
        final_response,
        ground_truth,
        llm_client
    )

    total_time = time.time() - input_complete_time

    return {
        "method": "baseline",
        "summary": full_text,  # æ²¡æœ‰æ€»ç»“
        "rag_results": results,
        "final_response": final_response,
        "timing": {
            "rag_time": rag_time,
            "ttft": ttft,  # time to first token
            "generation_time": gen_time,
            "total_time": total_time
        },
        "metrics": {
            "query_length": len(full_text),
            "response_length": len(final_response),
            "token_count": token_count,
            "tokens_per_second": token_count / gen_time if gen_time > 0 else 0
        },
        "evaluation": eval_result
    }


# ========== æ–¹æ³•2ï¼šå®Œæ•´æ€»ç»“åRAG ==========

def method2_batch_summary(
    full_text: str,
    vector_index: VectorIndex,
    embedding_service: EmbeddingService,
    llm_client: OpenAI,
    ground_truth: Dict,
    top_k: int = 5
) -> Dict:
    """æ–¹æ³•2ï¼šç­‰å¾…å®Œæ•´è¾“å…¥åä¸€æ¬¡æ€§æ€»ç»“ï¼Œç„¶åRAG"""
    # ç”¨æˆ·è¾“å…¥å®Œæˆæ—¶åˆ»
    input_complete_time = time.time()

    # 1. ä¸€æ¬¡æ€§æ€»ç»“
    summary_start = time.time()
    summary_prompt = f"""ç”¨æˆ·è¿›è¡Œäº†ä¸€æ®µè¯­éŸ³è¾“å…¥ï¼Œè¯·æ€»ç»“å…³é”®ä¿¡æ¯ï¼Œè¿‡æ»¤å£è¯­è¯å’Œå¯’æš„ã€‚

ç”¨æˆ·è¾“å…¥ï¼ˆ{len(full_text)}å­—ï¼‰ï¼š
{full_text}

åªè¿”å›ä¸€è¡Œç®€æ´çš„æ€»ç»“ï¼Œä¸è¦JSONï¼Œä¸è¦markdownï¼š"""

    response = llm_client.chat.completions.create(
        model=SUMMARY_MODEL,
        messages=[{"role": "user", "content": summary_prompt}],
        temperature=0.1,
        stream=False,
        extra_body={"enable_thinking": False}
    )

    summary = response.choices[0].message.content.strip()
    summary_time = time.time() - summary_start

    # 2. ä½¿ç”¨æ€»ç»“è¿›è¡ŒRAG
    rag_start = time.time()
    results = search_with_query_text(summary, vector_index, embedding_service, top_k)
    rag_time = time.time() - rag_start

    # 3. ç”Ÿæˆå›å¤
    rag_context = ""
    for i, doc in enumerate(results, 1):
        rag_context += f"\n[æ–‡æ¡£{i}] {doc.get('title', 'æ— æ ‡é¢˜')}\n"
        rag_context += f"{doc.get('content', 'æ— å†…å®¹')[:500]}\n"

    prompt = f"""ç”¨æˆ·åŸå§‹è¾“å…¥ï¼š
{full_text[:200]}...

æ€»ç»“ï¼š
{summary}

ç›¸å…³ä¿¡æ¯ï¼š
{rag_context}

è¯·ç»™å‡ºä¸“ä¸šã€å‡†ç¡®çš„å›å¤ï¼š
"""

    gen_start = time.time()
    final_response, ttft, gen_time, token_count = generate_response_streaming(prompt, llm_client)

    # 4. LLMè¯„ä¼°
    eval_result = llm_evaluate_all(
        "batch_summary",
        full_text,
        summary,
        results,
        final_response,
        ground_truth,
        llm_client
    )

    total_time = time.time() - input_complete_time

    return {
        "method": "batch_summary",
        "summary": summary,
        "rag_results": results,
        "final_response": final_response,
        "timing": {
            "summary_time": summary_time,
            "rag_time": rag_time,
            "ttft": ttft,
            "generation_time": gen_time,
            "total_time": total_time
        },
        "metrics": {
            "query_length": len(summary),
            "compression_ratio": len(summary) / len(full_text),
            "response_length": len(final_response),
            "token_count": token_count,
            "tokens_per_second": token_count / gen_time if gen_time > 0 else 0
        },
        "evaluation": eval_result
    }


# ========== æ–¹æ³•3ï¼šæ¸è¿›å¼æ€»ç»“+RAG ==========

def method3_incremental_summary(
    segments: List[Dict],
    vector_index: VectorIndex,
    embedding_service: EmbeddingService,
    llm_client: OpenAI,
    ground_truth: Dict,
    top_k: int = 5
) -> Dict:
    """æ–¹æ³•3ï¼šè¾¹è¾“å…¥è¾¹æ€»ç»“ï¼Œæœ€åè¿›è¡ŒRAG"""
    # 1. æ¸è¿›å¼æ€»ç»“ï¼ˆåŒ…å«æ¨¡æ‹Ÿå»¶è¿Ÿï¼‰
    summarizer = SimpleSummarizer(llm_client, model_name=SUMMARY_MODEL)
    segment_results = []

    summary_start_time = time.time()
    for segment_data in segments:
        seg_result = summarizer.add_segment(segment_data["text"], simulate_delay=True)
        segment_results.append(seg_result)

    # ç”¨æˆ·è¾“å…¥å®Œæˆæ—¶åˆ»ï¼ˆåŒ…å«è¯´è¯æ—¶é—´ï¼‰
    input_complete_time = time.time()

    summary = summarizer.get_final_summary()
    stats = summarizer.get_stats()

    # 2. ä½¿ç”¨æ€»ç»“è¿›è¡ŒRAG
    rag_start = time.time()
    results = search_with_query_text(summary, vector_index, embedding_service, top_k)
    rag_time = time.time() - rag_start

    # 3. ç”Ÿæˆå›å¤
    rag_context = ""
    for i, doc in enumerate(results, 1):
        rag_context += f"\n[æ–‡æ¡£{i}] {doc.get('title', 'æ— æ ‡é¢˜')}\n"
        rag_context += f"{doc.get('content', 'æ— å†…å®¹')[:500]}\n"

    full_text = "".join([seg["text"] for seg in segments])

    prompt = f"""ç”¨æˆ·è¿›è¡Œäº†{len(segments)}æ®µè¯­éŸ³è¾“å…¥ã€‚

æ€»ç»“ï¼š
{summary}

ç›¸å…³ä¿¡æ¯ï¼š
{rag_context}

è¯·ç»™å‡ºä¸“ä¸šã€å‡†ç¡®çš„å›å¤ï¼š
"""

    gen_start = time.time()
    final_response, ttft, gen_time, token_count = generate_response_streaming(prompt, llm_client)

    # 4. LLMè¯„ä¼°
    eval_result = llm_evaluate_all(
        "incremental_summary",
        full_text,
        summary,
        results,
        final_response,
        ground_truth,
        llm_client
    )

    # æ³¨æ„ï¼štotal_timeä»è¾“å…¥å®Œæˆå¼€å§‹è®¡ç®—
    total_time_after_input = time.time() - input_complete_time

    return {
        "method": "incremental_summary",
        "summary": summary,
        "rag_results": results,
        "final_response": final_response,
        "segment_results": segment_results,
        "timing": {
            "summary_time_with_speech": time.time() - summary_start_time,  # åŒ…å«è¯´è¯æ—¶é—´
            "summary_processing_time": stats["total_processing_time"],  # çº¯å¤„ç†æ—¶é—´
            "rag_time": rag_time,
            "ttft": ttft,
            "generation_time": gen_time,
            "total_time_after_input": total_time_after_input,  # è¾“å…¥å®Œæˆåçš„ç­‰å¾…æ—¶é—´
        },
        "metrics": {
            "query_length": len(summary),
            "compression_ratio": stats["compression_ratio"],
            "response_length": len(final_response),
            "token_count": token_count,
            "tokens_per_second": token_count / gen_time if gen_time > 0 else 0,
            "avg_segment_processing": stats["avg_segment_time"]
        },
        "evaluation": eval_result
    }


# ========== æ–¹æ³•4ï¼šæ¸è¿›å¼æ€»ç»“+å¢é‡RAGï¼ˆv3ï¼‰ ==========

def method4_incremental_rag(
    segments: List[Dict],
    vector_index: VectorIndex,
    embedding_service: EmbeddingService,
    llm_client: OpenAI,
    ground_truth: Dict,
    top_k: int = 5
) -> Dict:
    """æ–¹æ³•4ï¼šè¾¹è¾“å…¥è¾¹æ€»ç»“+è¾¹æ£€ç´¢ï¼Œè¿‡æ»¤ä½ç›¸å…³åº¦æ–‡æ¡£ï¼ˆv3ç‰ˆæœ¬ï¼‰"""
    # 1. æ¸è¿›å¼æ€»ç»“+RAGï¼ˆåŒ…å«æ¨¡æ‹Ÿå»¶è¿Ÿï¼‰
    summarizer = IncrementalRAGSummarizer(
        llm_client,
        embedding_service,
        vector_index,
        model_name=SUMMARY_MODEL,
        relevance_threshold=0.6
    )
    segment_results = []

    summary_start_time = time.time()
    for segment_data in segments:
        seg_result = summarizer.add_segment(segment_data["text"], simulate_delay=True)
        segment_results.append(seg_result)

    # ç”¨æˆ·è¾“å…¥å®Œæˆæ—¶åˆ»ï¼ˆåŒ…å«è¯´è¯æ—¶é—´ï¼‰
    input_complete_time = time.time()

    summary = summarizer.get_final_summary()
    stats = summarizer.get_stats()

    # è·å–ç´¯ç§¯çš„ç›¸å…³æ–‡æ¡£ï¼ˆå·²ç»å»é‡å’Œè¿‡æ»¤ï¼‰
    relevant_docs = summarizer.get_relevant_docs()

    # 2. ç”Ÿæˆå›å¤ï¼ˆä½¿ç”¨ç´¯ç§¯çš„ç›¸å…³æ–‡æ¡£ï¼‰
    rag_context = ""
    for i, doc in enumerate(relevant_docs[:top_k], 1):
        rag_context += f"\n[æ–‡æ¡£{i}] {doc.get('title', 'æ— æ ‡é¢˜')}\n"
        rag_context += f"{doc.get('content', 'æ— å†…å®¹')[:500]}\n"

    full_text = "".join([seg["text"] for seg in segments])

    prompt = f"""ç”¨æˆ·è¿›è¡Œäº†{len(segments)}æ®µè¯­éŸ³è¾“å…¥ã€‚

æ€»ç»“ï¼š
{summary}

ç›¸å…³ä¿¡æ¯ï¼š
{rag_context}

è¯·ç»™å‡ºä¸“ä¸šã€å‡†ç¡®çš„å›å¤ï¼š
"""

    gen_start = time.time()
    final_response, ttft, gen_time, token_count = generate_response_streaming(prompt, llm_client)

    # 3. LLMè¯„ä¼°
    eval_result = llm_evaluate_all(
        "incremental_rag_v3",
        full_text,
        summary,
        relevant_docs[:top_k],
        final_response,
        ground_truth,
        llm_client
    )

    # æ³¨æ„ï¼štotal_timeä»è¾“å…¥å®Œæˆå¼€å§‹è®¡ç®—
    total_time_after_input = time.time() - input_complete_time

    return {
        "method": "incremental_rag_v3",
        "summary": summary,
        "rag_results": relevant_docs[:top_k],
        "final_response": final_response,
        "segment_results": segment_results,
        "timing": {
            "summary_time_with_speech": time.time() - summary_start_time,  # åŒ…å«è¯´è¯æ—¶é—´
            "summary_processing_time": stats["total_processing_time"],  # çº¯å¤„ç†æ—¶é—´
            "rag_time": stats["total_rag_time"],  # ç´¯ç§¯RAGæ—¶é—´
            "avg_rag_time": stats["avg_rag_time"],  # å¹³å‡æ¯æ®µRAGæ—¶é—´
            "ttft": ttft,
            "generation_time": gen_time,
            "total_time_after_input": total_time_after_input,  # è¾“å…¥å®Œæˆåçš„ç­‰å¾…æ—¶é—´
        },
        "metrics": {
            "query_length": len(summary),
            "compression_ratio": stats["compression_ratio"],
            "response_length": len(final_response),
            "token_count": token_count,
            "tokens_per_second": token_count / gen_time if gen_time > 0 else 0,
            "avg_segment_processing": stats["avg_segment_time"],
            "total_retrieved_docs": stats["total_retrieved_docs"],
            "total_relevant_docs": stats["total_relevant_docs"]
        },
        "evaluation": eval_result
    }


# ========== ä¸»å®éªŒç±» ==========

class Experiment3V3Runner:
    """å®éªŒ3 v3è¿è¡Œå™¨"""

    def __init__(self):
        print("åˆå§‹åŒ–æœåŠ¡...")

        # LLMå®¢æˆ·ç«¯
        self.llm_client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

        # EmbeddingæœåŠ¡
        self.embedding_service = EmbeddingService()

        # åˆå§‹åŒ–çŸ¥è¯†åº“
        print("æ„å»ºçŸ¥è¯†åº“...")
        self.init_knowledge_base()

        print("âœ“ åˆå§‹åŒ–å®Œæˆ\n")

    def init_knowledge_base(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        company_docs = convert_all_companies_to_documents()
        all_docs = FICTIONAL_DOCUMENTS + company_docs

        self.vector_index = VectorIndex(self.embedding_service)
        self.vector_index.add_documents(all_docs)

        print(f"çŸ¥è¯†åº“æ–‡æ¡£æ•°: {len(all_docs)}")

    def run_single_test(self, test_case: Dict) -> Dict:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹ - å¹¶è¡Œæ‰§è¡Œå››ä¸ªæ–¹æ³•"""
        print(f"\n{'='*70}")
        print(f"æµ‹è¯•ç”¨ä¾‹: {test_case['id']}")
        print(f"ç±»åˆ«: {test_case['category']}")
        print(f"æ€»é•¿åº¦: {test_case['total_length']} å­—")
        print(f"åˆ†æ®µæ•°: {len(test_case['segments'])}")
        print(f"{'='*70}\n")

        full_text = "".join([seg["text"] for seg in test_case["segments"]])

        result = {
            "test_case_id": test_case["id"],
            "category": test_case["category"],
            "total_length": test_case["total_length"],
            "segment_count": len(test_case["segments"]),
            "ground_truth": test_case["ground_truth"]
        }

        print("ğŸš€ å¹¶è¡Œè¿è¡Œå››ä¸ªæ–¹æ³•...")
        start_parallel = time.time()

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè¿è¡Œå››ä¸ªæ–¹æ³•
        with ThreadPoolExecutor(max_workers=4) as executor:
            # æäº¤å››ä¸ªä»»åŠ¡
            future_m1 = executor.submit(
                method1_baseline,
                full_text,
                self.vector_index,
                self.embedding_service,
                self.llm_client,
                test_case["ground_truth"]
            )

            future_m2 = executor.submit(
                method2_batch_summary,
                full_text,
                self.vector_index,
                self.embedding_service,
                self.llm_client,
                test_case["ground_truth"]
            )

            future_m3 = executor.submit(
                method3_incremental_summary,
                test_case["segments"],
                self.vector_index,
                self.embedding_service,
                self.llm_client,
                test_case["ground_truth"]
            )

            future_m4 = executor.submit(
                method4_incremental_rag,
                test_case["segments"],
                self.vector_index,
                self.embedding_service,
                self.llm_client,
                test_case["ground_truth"]
            )

            # æ”¶é›†ç»“æœ
            futures = {
                "method1": future_m1,
                "method2": future_m2,
                "method3": future_m3,
                "method4": future_m4
            }

            for method_name, future in futures.items():
                try:
                    result_data = future.result()
                    if method_name == "method1":
                        result["method1_baseline"] = result_data
                        print(f"  âœ“ æ–¹æ³•1å®Œæˆï¼ˆ{result_data['timing']['total_time']:.2f}ç§’ï¼‰")
                    elif method_name == "method2":
                        result["method2_batch"] = result_data
                        print(f"  âœ“ æ–¹æ³•2å®Œæˆï¼ˆ{result_data['timing']['total_time']:.2f}ç§’ï¼‰")
                    elif method_name == "method3":
                        result["method3_incremental"] = result_data
                        print(f"  âœ“ æ–¹æ³•3å®Œæˆï¼ˆè¾“å…¥å: {result_data['timing']['total_time_after_input']:.2f}ç§’ï¼‰")
                    else:
                        result["method4_incremental_rag"] = result_data
                        print(f"  âœ“ æ–¹æ³•4å®Œæˆï¼ˆè¾“å…¥å: {result_data['timing']['total_time_after_input']:.2f}ç§’, æ£€ç´¢{result_data['metrics']['total_relevant_docs']}ä¸ªæ–‡æ¡£ï¼‰")
                except Exception as e:
                    print(f"  âœ— {method_name}å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()

        parallel_time = time.time() - start_parallel
        print(f"\nå¹¶è¡Œæ€»è€—æ—¶: {parallel_time:.2f}ç§’")

        return result

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        test_cases = load_test_cases()
        results = []

        print(f"\nå¼€å§‹è¿è¡Œ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹...")
        print(f"é…ç½®ï¼šæ€»ç»“æ¨¡å‹={SUMMARY_MODEL}, å›å¤æ¨¡å‹={RESPONSE_MODEL}")
        print(f"æ–¹æ³•å¯¹æ¯”ï¼š")
        print(f"  1. Baseline - ç›´æ¥RAG (800å­—åŸæ–‡)")
        print(f"  2. Batch Summary - ç­‰è¾“å…¥å®Œæˆåæ€»ç»“+RAG")
        print(f"  3. Incremental v2 - è¾¹è¾“å…¥è¾¹æ€»ç»“ï¼Œæœ€åRAG")
        print(f"  4. Incremental v3 (æ–°) - è¾¹è¾“å…¥è¾¹æ€»ç»“+å¢é‡RAGï¼Œç›¸å…³åº¦è¿‡æ»¤\n")

        for i, test_case in enumerate(test_cases, 1):
            print(f"\n[{i}/{len(test_cases)}]")
            try:
                result = self.run_single_test(test_case)
                results.append(result)
            except Exception as e:
                print(f"æµ‹è¯•å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

        # ä¿å­˜ç»“æœ
        self.save_results(results)

        return results

    def save_results(self, results: List[Dict]):
        """ä¿å­˜ç»“æœåˆ°JSON"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path(__file__).parent.parent / "outputs"
        output_dir.mkdir(exist_ok=True)

        output_file = output_dir / f"experiment3_v3_results_{timestamp}.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nâœ“ ç»“æœå·²ä¿å­˜: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    runner = Experiment3V3Runner()
    results = runner.run_all_tests()

    print("\n" + "="*70)
    print("å®éªŒ3 v3å®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    main()
