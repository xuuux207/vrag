"""
å®éªŒ3 v3 æœåŠ¡å™¨ç‰ˆæœ¬ï¼š4ç§æ–¹æ³•å¯¹æ¯”ï¼ˆä½¿ç”¨æœåŠ¡å™¨vLLMï¼‰

æµ‹è¯•ç›®æ ‡ï¼š
1. å¯¹æ¯”å››ç§æ–¹æ³•ï¼šç›´æ¥RAGã€æ‰¹é‡æ€»ç»“+RAGã€æ¸è¿›å¼æ€»ç»“v2+RAGã€æ¸è¿›å¼æ€»ç»“v3+å¢é‡RAG
2. éªŒè¯æ¸è¿›å¼æ€»ç»“åœ¨800å­—é•¿æ–‡æœ¬ä¸­çš„æ•ˆæœ
3. è¯„ä¼°å¢é‡RAGçš„ç›¸å…³åº¦è¿‡æ»¤å’Œå»é‡æ•ˆæœ

é…ç½®ï¼š
- LLM: æœåŠ¡å™¨æœ¬åœ° vLLM (Qwen/Qwen3-32B) - localhost:8000
- Embedding: ç¡…åŸºæµåŠ¨ API (BAAI/bge-m3)
- Reranking: ç¡…åŸºæµåŠ¨ API (BAAI/bge-reranker-v2-m3)
- å»¶è¿Ÿæ¨¡æ‹Ÿ: å…³é—­ï¼ˆæœåŠ¡å™¨ä¸éœ€è¦æ¨¡æ‹Ÿï¼‰
"""

import os
import sys
import json
import time
from datetime import datetime
from typing import Dict, List
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor

sys.path.insert(0, str(Path(__file__).parent.parent))

from openai import OpenAI
from dotenv import load_dotenv
from rag_utils import EmbeddingService, VectorIndex
from data.fictional_knowledge_base import FICTIONAL_DOCUMENTS
from data.company_graph import convert_all_companies_to_documents
from experiments.incremental_summarizer_v2 import SimpleSummarizer
from experiments.incremental_summarizer_v3 import IncrementalRAGSummarizer

load_dotenv()

# æ¨¡å‹é…ç½® - æœåŠ¡å™¨æœ¬åœ°vLLM
VLLM_BASE_URL = "http://localhost:8000/v1"
VLLM_MODEL = "Qwen/Qwen3-32B"

# ä½¿ç”¨ç»Ÿä¸€æ¨¡å‹ï¼ˆæœåŠ¡å™¨ä¸Šçš„Qwen3-32Bæ€§èƒ½è¶³å¤Ÿï¼‰
SUMMARY_MODEL = VLLM_MODEL
RESPONSE_MODEL = VLLM_MODEL

# Embeddingé…ç½®ï¼ˆç»§ç»­ä½¿ç”¨ç¡…åŸºæµåŠ¨ï¼‰
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
EMBEDDING_URL = os.getenv("EMBEDDING_URL", "https://api.siliconflow.cn/v1/embeddings")
EMBEDDING_TOKEN = os.getenv("EMBEDDING_TOKEN")


# ========== åŠ è½½æµ‹è¯•ç”¨ä¾‹ ==========

def load_test_cases() -> List[Dict]:
    """åŠ è½½æµ‹è¯•ç”¨ä¾‹ï¼ˆv2ç‰ˆæœ¬ï¼ŒåŒ…å«åˆ†æ®µå’Œå¹²æ‰°é¡¹ï¼‰"""
    test_file = Path(__file__).parent / "long_audio_test_cases_v2.json"
    with open(test_file, 'r', encoding='utf-8') as f:
        data = json.loads(f.read())
    return data["test_cases"]


# ========== RAGå·¥å…·å‡½æ•° ==========

def search_with_query_text(
    query_text: str,
    vector_index: VectorIndex,
    embedding_service: EmbeddingService,
    top_k: int = 5
) -> List[Dict]:
    """ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢è¿›è¡Œå‘é‡æ£€ç´¢"""
    query_vector = embedding_service.embed_single(query_text)
    results = vector_index.search(query_vector, top_k=top_k)
    return results


# ========== æµå¼ç”Ÿæˆ ==========

def generate_response_streaming(prompt: str, llm_client: OpenAI) -> tuple:
    """
    æµå¼ç”Ÿæˆå›å¤ï¼Œè¿”å›(å®Œæ•´å›å¤, TTFT, ç”Ÿæˆæ—¶é—´, tokenæ•°é‡)
    """
    start_time = time.time()
    ttft = None
    response_text = ""
    token_count = 0

    stream = llm_client.chat.completions.create(
        model=RESPONSE_MODEL,
        messages=[{"role": "user", "content": prompt}],
        stream=True,
        max_tokens=2000,
        temperature=0.7
    )

    for chunk in stream:
        if chunk.choices and chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            if ttft is None:
                ttft = time.time() - start_time
            response_text += content
            token_count += 1

    generation_time = time.time() - start_time

    if ttft is None:
        ttft = generation_time

    return response_text, ttft, generation_time, token_count


# ========== LLMè¯„ä¼°å‡½æ•° ==========

def llm_evaluate_all(
    method_name: str,
    full_text: str,
    summary: str,
    rag_results: List[Dict],
    final_response: str,
    ground_truth: Dict,
    llm_client: OpenAI
) -> Dict:
    """ä½¿ç”¨LLMè¯„ä¼°æ‰€æœ‰ç»´åº¦"""
    eval_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­éŸ³åŠ©æ‰‹è¯„ä¼°ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç»™å‡ºè¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰ã€‚

ã€åŸå§‹è¾“å…¥ã€‘ï¼ˆç”¨æˆ·çš„å®Œæ•´è¯­éŸ³è¾“å…¥ï¼Œçº¦800å­—ï¼‰ï¼š
{full_text}

ã€æ€»ç»“ã€‘ï¼ˆå¦‚æœæœ‰ï¼‰ï¼š
{summary if summary else "ï¼ˆæ— æ€»ç»“ï¼Œç›´æ¥ä½¿ç”¨åŸæ–‡ï¼‰"}

ã€RAGæ£€ç´¢ç»“æœã€‘ï¼š
{len(rag_results)}ä¸ªæ–‡æ¡£è¢«æ£€ç´¢åˆ°
{chr(10).join([f"- {doc.get('title', 'æ— æ ‡é¢˜')}" for doc in rag_results[:3]])}

ã€æœ€ç»ˆå›å¤ã€‘ï¼š
{final_response}

ã€Ground Truthã€‘ï¼š
ç”¨æˆ·çš„å…³é”®éœ€æ±‚ï¼š{ground_truth.get('key_points', [])}
åº”è¯¥é¿å…çš„å™ªéŸ³ï¼š{ground_truth.get('noise_patterns', [])}

è¯·ä»ä»¥ä¸‹5ä¸ªç»´åº¦è¯„åˆ†ï¼š

1. **ä¿¡æ¯ä¿ç•™ç‡** (0-100åˆ†)
   - æœ€ç»ˆå›å¤æ˜¯å¦è¦†ç›–äº†ç”¨æˆ·çš„æ‰€æœ‰å…³é”®éœ€æ±‚ç‚¹ï¼Ÿ
   - é‡è¦çš„ç»†èŠ‚ï¼ˆé¢„ç®—ã€æ—¶é—´ã€æŠ€æœ¯è¦æ±‚ç­‰ï¼‰æ˜¯å¦éƒ½è¢«ä¿ç•™ï¼Ÿ

2. **å™ªéŸ³è¿‡æ»¤ç‡** (0-100åˆ†)
   - æ˜¯å¦æˆåŠŸè¿‡æ»¤äº†å£è¯­åŒ–è¡¨è¾¾ã€é‡å¤ã€æ— å…³é—²èŠï¼Ÿ
   - æ€»ç»“/å›å¤æ˜¯å¦ç®€æ´ä¸“ä¸šï¼Ÿ

3. **RAGç›¸å…³æ€§** (0-100åˆ†)
   - æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦ä¸ç”¨æˆ·éœ€æ±‚é«˜åº¦ç›¸å…³ï¼Ÿ
   - å›å¤ä¸­çš„ä¿¡æ¯æ˜¯å¦æ¥è‡ªç›¸å…³æ–‡æ¡£ï¼Ÿ

4. **å›å¤è´¨é‡** (0-100åˆ†)
   - å›å¤æ˜¯å¦å‡†ç¡®ã€ä¸“ä¸šã€æœ‰é’ˆå¯¹æ€§ï¼Ÿ
   - æ˜¯å¦å›ç­”äº†ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜ï¼Ÿ

5. **ç®€æ´åº¦** (0-100åˆ†)
   - å›å¤æ˜¯å¦ç®€æ´æ˜äº†ï¼Œæ²¡æœ‰å†—ä½™ï¼Ÿ
   - ä¿¡æ¯å¯†åº¦æ˜¯å¦åˆç†ï¼Ÿ

è¿”å›JSONæ ¼å¼ï¼ˆä¸è¦markdownæ ‡è®°ï¼Œä¸è¦ç¤ºä¾‹æ•°å­—ï¼Œç»™å‡ºä½ çš„çœŸå®è¯„åˆ†ï¼‰ï¼š
{{
  "info_retention_score": <ä½ çš„è¯„åˆ†>,
  "noise_filtering_score": <ä½ çš„è¯„åˆ†>,
  "rag_relevance_score": <ä½ çš„è¯„åˆ†>,
  "response_quality_score": <ä½ çš„è¯„åˆ†>,
  "conciseness_score": <ä½ çš„è¯„åˆ†>,
  "total_score": <5é¡¹å¹³å‡å€¼>,
  "reasoning": "<è¯¦ç»†çš„è¯„åˆ†ç†ç”±>"
}}
"""

    response = llm_client.chat.completions.create(
        model=RESPONSE_MODEL,
        messages=[{"role": "user", "content": eval_prompt}],
        temperature=0.3,
        max_tokens=1000
    )

    result_text = response.choices[0].message.content.strip()

    # å»é™¤markdownä»£ç å—æ ‡è®°
    if result_text.startswith("```"):
        lines = result_text.split('\n')
        result_text = '\n'.join(lines[1:-1])

    try:
        result = json.loads(result_text)
        return result
    except:
        print(f"[{method_name}] LLMè¯„ä¼°è¿”å›æ ¼å¼é”™è¯¯: {result_text[:200]}")
        return {
            "info_retention_score": 0,
            "noise_filtering_score": 0,
            "rag_relevance_score": 0,
            "response_quality_score": 0,
            "conciseness_score": 0,
            "total_score": 0,
            "reasoning": "è¯„ä¼°å¤±è´¥"
        }


# ========== æ–¹æ³•1ï¼šç›´æ¥RAGï¼ˆBaselineï¼‰ ==========

def method1_baseline(
    full_text: str,
    vector_index: VectorIndex,
    embedding_service: EmbeddingService,
    llm_client: OpenAI,
    ground_truth: Dict,
    top_k: int = 5
) -> Dict:
    """æ–¹æ³•1ï¼šç›´æ¥ä½¿ç”¨å®Œæ•´æ–‡æœ¬è¿›è¡ŒRAG"""
    start_time = time.time()

    # 1. RAGæ£€ç´¢
    rag_start = time.time()
    results = search_with_query_text(full_text, vector_index, embedding_service, top_k)
    rag_time = time.time() - rag_start

    # 2. æ„å»ºcontext
    rag_context = ""
    for i, doc in enumerate(results, 1):
        rag_context += f"\n[æ–‡æ¡£{i}] {doc.get('title', 'æ— æ ‡é¢˜')}\n"
        rag_context += f"{doc.get('content', 'æ— å†…å®¹')[:500]}\n"

    # 3. ç”Ÿæˆå›å¤
    prompt = f"""ç”¨æˆ·çš„è¯­éŸ³è¾“å…¥ï¼š
{full_text}

ç›¸å…³ä¿¡æ¯ï¼š
{rag_context}

è¯·ç»™å‡ºä¸“ä¸šã€å‡†ç¡®çš„å›å¤ï¼š
"""

    gen_start = time.time()
    final_response, ttft, gen_time, token_count = generate_response_streaming(prompt, llm_client)

    # 4. LLMè¯„ä¼°
    eval_result = llm_evaluate_all(
        "baseline",
        full_text,
        "",
        results,
        final_response,
        ground_truth,
        llm_client
    )

    total_time = time.time() - start_time

    return {
        "method": "baseline",
        "summary": "",
        "rag_results": results,
        "final_response": final_response,
        "timing": {
            "rag_time": rag_time,
            "ttft": ttft,
            "generation_time": gen_time,
            "total_time": total_time
        },
        "metrics": {
            "query_length": len(full_text),
            "response_length": len(final_response),
            "token_count": token_count,
            "tokens_per_second": token_count / gen_time if gen_time > 0 else 0
        },
        "evaluation": eval_result
    }


# ========== æ–¹æ³•2ï¼šæ‰¹é‡æ€»ç»“+RAG ==========

def method2_batch_summary(
    full_text: str,
    vector_index: VectorIndex,
    embedding_service: EmbeddingService,
    llm_client: OpenAI,
    ground_truth: Dict,
    top_k: int = 5
) -> Dict:
    """æ–¹æ³•2ï¼šç­‰å¾…è¾“å…¥å®Œæˆåï¼Œæ‰¹é‡æ€»ç»“ï¼Œç„¶åRAG"""
    start_time = time.time()

    # 1. æ‰¹é‡æ€»ç»“
    summary_start = time.time()
    summary_prompt = f"""è¯·æ€»ç»“ä»¥ä¸‹è¯­éŸ³è¾“å…¥çš„æ ¸å¿ƒéœ€æ±‚ï¼Œè¿‡æ»¤å£è¯­è¯å’Œæ— å…³å†…å®¹ï¼š

{full_text}

åªè¿”å›ç®€æ´çš„æ€»ç»“ï¼ˆä¸€æ®µè¯ï¼‰ï¼š
"""

    summary_response = llm_client.chat.completions.create(
        model=SUMMARY_MODEL,
        messages=[{"role": "user", "content": summary_prompt}],
        temperature=0.3,
        max_tokens=500
    )
    summary = summary_response.choices[0].message.content.strip()
    summary_time = time.time() - summary_start

    # 2. RAGæ£€ç´¢
    rag_start = time.time()
    results = search_with_query_text(summary, vector_index, embedding_service, top_k)
    rag_time = time.time() - rag_start

    # 3. ç”Ÿæˆå›å¤
    rag_context = ""
    for i, doc in enumerate(results, 1):
        rag_context += f"\n[æ–‡æ¡£{i}] {doc.get('title', 'æ— æ ‡é¢˜')}\n"
        rag_context += f"{doc.get('content', 'æ— å†…å®¹')[:500]}\n"

    prompt = f"""ç”¨æˆ·éœ€æ±‚æ€»ç»“ï¼š
{summary}

ç›¸å…³ä¿¡æ¯ï¼š
{rag_context}

è¯·ç»™å‡ºä¸“ä¸šã€å‡†ç¡®çš„å›å¤ï¼š
"""

    gen_start = time.time()
    final_response, ttft, gen_time, token_count = generate_response_streaming(prompt, llm_client)

    # 4. LLMè¯„ä¼°
    eval_result = llm_evaluate_all(
        "batch_summary",
        full_text,
        summary,
        results,
        final_response,
        ground_truth,
        llm_client
    )

    total_time = time.time() - start_time

    return {
        "method": "batch_summary",
        "summary": summary,
        "rag_results": results,
        "final_response": final_response,
        "timing": {
            "summary_time": summary_time,
            "rag_time": rag_time,
            "ttft": ttft,
            "generation_time": gen_time,
            "total_time": total_time
        },
        "metrics": {
            "query_length": len(summary),
            "compression_ratio": len(summary) / len(full_text) if len(full_text) > 0 else 0,
            "response_length": len(final_response),
            "token_count": token_count,
            "tokens_per_second": token_count / gen_time if gen_time > 0 else 0
        },
        "evaluation": eval_result
    }


# ========== æ–¹æ³•3ï¼šæ¸è¿›å¼æ€»ç»“+RAG ==========

def method3_incremental_summary(
    segments: List[Dict],
    vector_index: VectorIndex,
    embedding_service: EmbeddingService,
    llm_client: OpenAI,
    ground_truth: Dict,
    top_k: int = 5
) -> Dict:
    """æ–¹æ³•3ï¼šè¾¹è¾“å…¥è¾¹æ€»ç»“ï¼Œæœ€åè¿›è¡ŒRAG"""
    # 1. æ¸è¿›å¼æ€»ç»“ï¼ˆä¸æ¨¡æ‹Ÿå»¶è¿Ÿï¼‰
    summarizer = SimpleSummarizer(llm_client, model_name=SUMMARY_MODEL)
    segment_results = []

    summary_start_time = time.time()
    for segment_data in segments:
        seg_result = summarizer.add_segment(segment_data["text"], simulate_delay=False)
        segment_results.append(seg_result)

    # ç”¨æˆ·è¾“å…¥å®Œæˆæ—¶åˆ»
    input_complete_time = time.time()

    summary = summarizer.get_final_summary()
    stats = summarizer.get_stats()

    # 2. ä½¿ç”¨æ€»ç»“è¿›è¡ŒRAG
    rag_start = time.time()
    results = search_with_query_text(summary, vector_index, embedding_service, top_k)
    rag_time = time.time() - rag_start

    # 3. ç”Ÿæˆå›å¤
    rag_context = ""
    for i, doc in enumerate(results, 1):
        rag_context += f"\n[æ–‡æ¡£{i}] {doc.get('title', 'æ— æ ‡é¢˜')}\n"
        rag_context += f"{doc.get('content', 'æ— å†…å®¹')[:500]}\n"

    full_text = "".join([seg["text"] for seg in segments])

    prompt = f"""ç”¨æˆ·è¿›è¡Œäº†{len(segments)}æ®µè¯­éŸ³è¾“å…¥ã€‚

æ€»ç»“ï¼š
{summary}

ç›¸å…³ä¿¡æ¯ï¼š
{rag_context}

è¯·ç»™å‡ºä¸“ä¸šã€å‡†ç¡®çš„å›å¤ï¼š
"""

    gen_start = time.time()
    final_response, ttft, gen_time, token_count = generate_response_streaming(prompt, llm_client)

    # 4. LLMè¯„ä¼°
    eval_result = llm_evaluate_all(
        "incremental_summary",
        full_text,
        summary,
        results,
        final_response,
        ground_truth,
        llm_client
    )

    # æ³¨æ„ï¼štotal_timeä»è¾“å…¥å®Œæˆå¼€å§‹è®¡ç®—
    total_time_after_input = time.time() - input_complete_time

    return {
        "method": "incremental_summary",
        "summary": summary,
        "rag_results": results,
        "final_response": final_response,
        "segment_results": segment_results,
        "timing": {
            "summary_time_total": time.time() - summary_start_time,
            "summary_processing_time": stats["total_processing_time"],
            "rag_time": rag_time,
            "ttft": ttft,
            "generation_time": gen_time,
            "total_time_after_input": total_time_after_input,
        },
        "metrics": {
            "query_length": len(summary),
            "compression_ratio": stats["compression_ratio"],
            "response_length": len(final_response),
            "token_count": token_count,
            "tokens_per_second": token_count / gen_time if gen_time > 0 else 0,
            "avg_segment_processing": stats["avg_segment_time"]
        },
        "evaluation": eval_result
    }


# ========== æ–¹æ³•4ï¼šæ¸è¿›å¼æ€»ç»“+å¢é‡RAGï¼ˆv3ï¼‰ ==========

def method4_incremental_rag(
    segments: List[Dict],
    vector_index: VectorIndex,
    embedding_service: EmbeddingService,
    llm_client: OpenAI,
    ground_truth: Dict,
    top_k: int = 5
) -> Dict:
    """æ–¹æ³•4ï¼šè¾¹è¾“å…¥è¾¹æ€»ç»“+è¾¹æ£€ç´¢ï¼Œè¿‡æ»¤ä½ç›¸å…³åº¦æ–‡æ¡£ï¼ˆv3ç‰ˆæœ¬ï¼‰"""
    # 1. æ¸è¿›å¼æ€»ç»“+RAGï¼ˆä¸æ¨¡æ‹Ÿå»¶è¿Ÿï¼‰
    summarizer = IncrementalRAGSummarizer(
        llm_client,
        embedding_service,
        vector_index,
        model_name=SUMMARY_MODEL,
        relevance_threshold=0.6
    )
    segment_results = []

    summary_start_time = time.time()
    for segment_data in segments:
        seg_result = summarizer.add_segment(segment_data["text"], simulate_delay=False)
        segment_results.append(seg_result)

    # ç”¨æˆ·è¾“å…¥å®Œæˆæ—¶åˆ»
    input_complete_time = time.time()

    summary = summarizer.get_final_summary()
    stats = summarizer.get_stats()

    # è·å–ç´¯ç§¯çš„ç›¸å…³æ–‡æ¡£ï¼ˆå·²ç»å»é‡å’Œè¿‡æ»¤ï¼‰
    relevant_docs = summarizer.get_relevant_docs()

    # 2. ç”Ÿæˆå›å¤ï¼ˆä½¿ç”¨ç´¯ç§¯çš„ç›¸å…³æ–‡æ¡£ï¼‰
    rag_context = ""
    for i, doc in enumerate(relevant_docs[:top_k], 1):
        rag_context += f"\n[æ–‡æ¡£{i}] {doc.get('title', 'æ— æ ‡é¢˜')}\n"
        rag_context += f"{doc.get('content', 'æ— å†…å®¹')[:500]}\n"

    full_text = "".join([seg["text"] for seg in segments])

    prompt = f"""ç”¨æˆ·è¿›è¡Œäº†{len(segments)}æ®µè¯­éŸ³è¾“å…¥ã€‚

æ€»ç»“ï¼š
{summary}

ç›¸å…³ä¿¡æ¯ï¼š
{rag_context}

è¯·ç»™å‡ºä¸“ä¸šã€å‡†ç¡®çš„å›å¤ï¼š
"""

    gen_start = time.time()
    final_response, ttft, gen_time, token_count = generate_response_streaming(prompt, llm_client)

    # 3. LLMè¯„ä¼°
    eval_result = llm_evaluate_all(
        "incremental_rag_v3",
        full_text,
        summary,
        relevant_docs[:top_k],
        final_response,
        ground_truth,
        llm_client
    )

    # æ³¨æ„ï¼štotal_timeä»è¾“å…¥å®Œæˆå¼€å§‹è®¡ç®—
    total_time_after_input = time.time() - input_complete_time

    return {
        "method": "incremental_rag_v3",
        "summary": summary,
        "rag_results": relevant_docs[:top_k],
        "final_response": final_response,
        "segment_results": segment_results,
        "timing": {
            "summary_time_total": time.time() - summary_start_time,
            "summary_processing_time": stats["total_processing_time"],
            "rag_time": stats["total_rag_time"],
            "avg_rag_time": stats["avg_rag_time"],
            "ttft": ttft,
            "generation_time": gen_time,
            "total_time_after_input": total_time_after_input,
        },
        "metrics": {
            "query_length": len(summary),
            "compression_ratio": stats["compression_ratio"],
            "response_length": len(final_response),
            "token_count": token_count,
            "tokens_per_second": token_count / gen_time if gen_time > 0 else 0,
            "avg_segment_processing": stats["avg_segment_time"],
            "total_retrieved_docs": stats["total_retrieved_docs"],
            "total_relevant_docs": stats["total_relevant_docs"]
        },
        "evaluation": eval_result
    }


# ========== ä¸»å®éªŒç±» ==========

class Experiment3V3Runner:
    """å®éªŒ3 v3è¿è¡Œå™¨ï¼ˆæœåŠ¡å™¨ç‰ˆæœ¬ï¼‰"""

    def __init__(self):
        print("\nåˆå§‹åŒ–å®éªŒ3 v3ï¼ˆæœåŠ¡å™¨ç‰ˆæœ¬ï¼‰...")
        print(f"LLM: {VLLM_MODEL} @ {VLLM_BASE_URL}")
        print(f"Embedding: {EMBEDDING_MODEL}")

        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨æœ¬åœ°vLLMï¼‰
        self.llm_client = OpenAI(
            base_url=VLLM_BASE_URL,
            api_key="EMPTY"  # vLLMä¸éœ€è¦API key
        )

        # åˆå§‹åŒ–EmbeddingæœåŠ¡ï¼ˆä½¿ç”¨ç¡…åŸºæµåŠ¨ï¼‰
        self.embedding_service = EmbeddingService(
            model_name=EMBEDDING_MODEL,
            api_key=EMBEDDING_TOKEN,
            base_url=EMBEDDING_URL
        )

        # åˆå§‹åŒ–å‘é‡ç´¢å¼•
        self.vector_index = VectorIndex(self.embedding_service)

        # åŠ è½½çŸ¥è¯†åº“
        all_docs = FICTIONAL_DOCUMENTS + convert_all_companies_to_documents()
        self.vector_index.add_documents(all_docs)

        print(f"çŸ¥è¯†åº“æ–‡æ¡£æ•°: {len(all_docs)}")

    def run_single_test(self, test_case: Dict) -> Dict:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹ - å¹¶è¡Œæ‰§è¡Œå››ä¸ªæ–¹æ³•"""
        print(f"\n{'='*70}")
        print(f"æµ‹è¯•ç”¨ä¾‹: {test_case['id']}")
        print(f"ç±»åˆ«: {test_case['category']}")
        print(f"æ€»é•¿åº¦: {test_case['total_length']} å­—")
        print(f"åˆ†æ®µæ•°: {len(test_case['segments'])}")
        print(f"{'='*70}\n")

        full_text = "".join([seg["text"] for seg in test_case["segments"]])

        result = {
            "test_case_id": test_case["id"],
            "category": test_case["category"],
            "total_length": test_case["total_length"],
            "segment_count": len(test_case["segments"]),
            "ground_truth": test_case["ground_truth"]
        }

        print("ğŸš€ å¹¶è¡Œè¿è¡Œå››ä¸ªæ–¹æ³•...")
        start_parallel = time.time()

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè¿è¡Œå››ä¸ªæ–¹æ³•
        with ThreadPoolExecutor(max_workers=4) as executor:
            # æäº¤å››ä¸ªä»»åŠ¡
            future_m1 = executor.submit(
                method1_baseline,
                full_text,
                self.vector_index,
                self.embedding_service,
                self.llm_client,
                test_case["ground_truth"]
            )

            future_m2 = executor.submit(
                method2_batch_summary,
                full_text,
                self.vector_index,
                self.embedding_service,
                self.llm_client,
                test_case["ground_truth"]
            )

            future_m3 = executor.submit(
                method3_incremental_summary,
                test_case["segments"],
                self.vector_index,
                self.embedding_service,
                self.llm_client,
                test_case["ground_truth"]
            )

            future_m4 = executor.submit(
                method4_incremental_rag,
                test_case["segments"],
                self.vector_index,
                self.embedding_service,
                self.llm_client,
                test_case["ground_truth"]
            )

            # æ”¶é›†ç»“æœ
            futures = {
                "method1": future_m1,
                "method2": future_m2,
                "method3": future_m3,
                "method4": future_m4
            }

            for method_name, future in futures.items():
                try:
                    result_data = future.result()
                    if method_name == "method1":
                        result["method1_baseline"] = result_data
                        print(f"  âœ“ æ–¹æ³•1å®Œæˆï¼ˆ{result_data['timing']['total_time']:.2f}ç§’ï¼‰")
                    elif method_name == "method2":
                        result["method2_batch"] = result_data
                        print(f"  âœ“ æ–¹æ³•2å®Œæˆï¼ˆ{result_data['timing']['total_time']:.2f}ç§’ï¼‰")
                    elif method_name == "method3":
                        result["method3_incremental"] = result_data
                        print(f"  âœ“ æ–¹æ³•3å®Œæˆï¼ˆè¾“å…¥å: {result_data['timing']['total_time_after_input']:.2f}ç§’ï¼‰")
                    else:
                        result["method4_incremental_rag"] = result_data
                        print(f"  âœ“ æ–¹æ³•4å®Œæˆï¼ˆè¾“å…¥å: {result_data['timing']['total_time_after_input']:.2f}ç§’, æ£€ç´¢{result_data['metrics']['total_relevant_docs']}ä¸ªæ–‡æ¡£ï¼‰")
                except Exception as e:
                    print(f"  âœ— {method_name}å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()

        parallel_time = time.time() - start_parallel
        print(f"\nå¹¶è¡Œæ€»è€—æ—¶: {parallel_time:.2f}ç§’")

        return result

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        test_cases = load_test_cases()
        results = []

        print(f"\nå¼€å§‹è¿è¡Œ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹...")
        print(f"é…ç½®ï¼šæ€»ç»“æ¨¡å‹={SUMMARY_MODEL}, å›å¤æ¨¡å‹={RESPONSE_MODEL}")
        print(f"æ–¹æ³•å¯¹æ¯”ï¼š")
        print(f"  1. Baseline - ç›´æ¥RAG (800å­—åŸæ–‡)")
        print(f"  2. Batch Summary - ç­‰è¾“å…¥å®Œæˆåæ€»ç»“+RAG")
        print(f"  3. Incremental v2 - è¾¹è¾“å…¥è¾¹æ€»ç»“ï¼Œæœ€åRAG")
        print(f"  4. Incremental v3 (æ–°) - è¾¹è¾“å…¥è¾¹æ€»ç»“+å¢é‡RAGï¼Œç›¸å…³åº¦è¿‡æ»¤\n")

        for i, test_case in enumerate(test_cases, 1):
            print(f"\n[{i}/{len(test_cases)}]")
            try:
                result = self.run_single_test(test_case)
                results.append(result)
            except Exception as e:
                print(f"æµ‹è¯•å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

        # ä¿å­˜ç»“æœ
        output_dir = Path(__file__).parent.parent / "outputs"
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"experiment3_v3_server_results_{timestamp}.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… å®éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return results


def main():
    runner = Experiment3V3Runner()
    results = runner.run_all_tests()


if __name__ == "__main__":
    main()
